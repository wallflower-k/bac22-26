{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8d565-65b0-434d-8165-29983ef556c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Задача 1 (5 баллов). \n",
    "\n",
    "Представим себе, что мы хотим подготовить корпус текстов для морфосинтаксической разметки и дальнейшего поиска по ним. Нам необходимо:\n",
    "\n",
    "1. Подобрать данные - наш корпус игрушечный, но вам необходимо выбрать столько текстов, чтобы суммарно в них было не меньше 10 тысяч слов. Объясните свой выбор текстов: если не будет объяснения, минус балл. Можете пользоваться wikipedia-api или даже поискать готовые датасеты текстов (язык не имеет большого значения, можно взять что-то кроме русского).\n",
    "2. Удостовериться, что тексты чистые, в них нет ссылок, хештегов, мусора, оставшегося после html-обвязки, и подобного. Если что-то из этого есть, почистить с помощью регулярных выражений. \n",
    "3. Посчитать статистику для нашего корпуса и красиво ее представить (с помощью f-строк): сколько в корпусе документов, сколько в каждом документе слов, сколько предложений. \n",
    "4. Вывести распределение предложений по длинам: сколько в корпусе в процентах предложений длиной до 10 слов, от 11 до 20 слов и так далее, а также сколько слов суммарно приходится на предложения каждой категории длины. Вывод должен выглядеть примерно так:\n",
    "\n",
    "        1-10 слов:\n",
    "        1234 слова всего\n",
    "        4% (12 предложений)\n",
    "        11-20 слов:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53960671-0cb1-4f43-97a2-8e506066623a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего текстов в корпусе: 6\n",
      "1. Bir başka Orhan.\n",
      "Предложений: 69\n",
      "Слов: 1327\n",
      "Всего текстов в корпусе: 6\n",
      "2. Karanlık müze evin fotoğrafları.\n",
      "Предложений: 71\n",
      "Слов: 2093\n",
      "Всего текстов в корпусе: 6\n",
      "3. Ben.\n",
      "Предложений: 77\n",
      "Слов: 1990\n",
      "Всего текстов в корпусе: 6\n",
      "4. Yıkılan paşa konaklarının hüznü. Sokakların keşfi.\n",
      "Предложений: 60\n",
      "Слов: 1557\n",
      "Всего текстов в корпусе: 6\n",
      "5. Siyah-Beyaz.\n",
      "Предложений: 67\n",
      "Слов: 2264\n",
      "Всего текстов в корпусе: 6\n",
      "6. Boğaz'ın keşfi.\n",
      "Предложений: 65\n",
      "Слов: 2345\n"
     ]
    }
   ],
   "source": [
    "# выбрала часть глав из книги орхана памука. во-первых, он нейтив спикер, поэтому там есть сложные конструкции, во-вторых, я просто так захотела. ну и еще удобно, когда автор один для статистики\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+')\n",
    "total_data = ''\n",
    "files = os.listdir('data_turkish')\n",
    "for f in files:\n",
    "    print(f'Всего текстов в корпусе: {len(files)}')\n",
    "    text = 'data_turkish/' + f\n",
    "    print(f[:-3])\n",
    "    with open(text, 'r', encoding='utf8') as file:\n",
    "        raw = file.read()\n",
    "        print(f'Предложений: {len(sent_tokenize(raw))}')\n",
    "        print(f'Слов: {len(tokenizer.tokenize(raw))}')\n",
    "        total_data += raw\n",
    "\n",
    "# tokens = tokenizer.tokenize(total_data)\n",
    "sentences = sent_tokenize(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd9e6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    if len(tokens) > max:\n",
    "        max = len(tokens)\n",
    "        dam = sentence\n",
    "\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cbe5f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-10 слов:\n",
      "372 слова всего\n",
      "0.13% (52 предложений)\n",
      "11-20 слов:\n",
      "1626 слова всего\n",
      "0.26% (105 предложений)\n",
      "21-30 слов:\n",
      "2840 слова всего\n",
      "0.28% (113 предложений)\n",
      "31-40 слов:\n",
      "1918 слова всего\n",
      "0.14% (56 предложений)\n",
      "41-50 слов:\n",
      "1725 слова всего\n",
      "0.1% (39 предложений)\n",
      "51-60 слов:\n",
      "765 слова всего\n",
      "0.03% (14 предложений)\n",
      "61-70 слов:\n",
      "926 слова всего\n",
      "0.03% (14 предложений)\n",
      "71-80 слов:\n",
      "225 слова всего\n",
      "0.01% (3 предложений)\n",
      "81-90 слов:\n",
      "435 слова всего\n",
      "0.01% (5 предложений)\n",
      "91-100 слов:\n",
      "184 слова всего\n",
      "0.0% (2 предложений)\n",
      "101-110 слов:\n",
      "0 слова всего\n",
      "0.0% (0 предложений)\n",
      "111-120 слов:\n",
      "119 слова всего\n",
      "0.0% (1 предложений)\n",
      "121-130 слов:\n",
      "248 слова всего\n",
      "0.0% (2 предложений)\n",
      "131-140 слов:\n",
      "0 слова всего\n",
      "0.0% (0 предложений)\n",
      "141-150 слов:\n",
      "0 слова всего\n",
      "0.0% (0 предложений)\n",
      "151-160 слов:\n",
      "0 слова всего\n",
      "0.0% (0 предложений)\n",
      "161-170 слов:\n",
      "0 слова всего\n",
      "0.0% (0 предложений)\n",
      "171-180 слов:\n",
      "0 слова всего\n",
      "0.0% (0 предложений)\n",
      "181-190 слов:\n",
      "0 слова всего\n",
      "0.0% (0 предложений)\n",
      "191-200 слов:\n",
      "193 слова всего\n",
      "0.0% (1 предложений)\n"
     ]
    }
   ],
   "source": [
    "total_sent = len(sentences)\n",
    "statistics = dict()\n",
    "start = 10\n",
    "for i in range(10, 201, 10):\n",
    "    statistics[str(i)] = list()\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    key = ((len(tokens)-1)//10 + 1)*10\n",
    "    statistics[str(key)].append(tokens)\n",
    "for num in statistics.keys():\n",
    "    print(f'{int(num)-9}-{int(num)} слов:')\n",
    "    amount = 0\n",
    "    for sent in statistics[num]:\n",
    "        amount += len(sent)\n",
    "    print(f'{amount} слова всего')\n",
    "    print(f'{round(len(statistics[num])/total_sent, 2)}% ({len(statistics[num])} предложений)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
