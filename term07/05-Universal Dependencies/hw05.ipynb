{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49240231-3107-4250-a39b-438a6d4b1182",
   "metadata": {},
   "source": [
    "Для выполнения всех следующих задач вам необходимо выбрать два текстовых файла примерно одинаковой длины и похожего жанра на двух разных языках, будет неплохо, если это окажется один и тот же текст в переводе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72179ba4-eb0c-400a-bbda-518afe58eb31",
   "metadata": {},
   "source": [
    "#### Задача 1. \n",
    "\n",
    "Просмотрите оба выбранных текста. Удостоверьтесь, что тексты чистые, если же в них есть какой-то мусор: хештеги, затесавшиеся при OCR символы и подобное, почистите с помощью регулярных выражений. \n",
    "\n",
    "Проведите первичный статистический анализ: разбейте тексты на предложения и на токены, посчитайте количество того и другого, сопоставьте. Какой текст оказался длиннее? В каком тексте средняя длина предложения больше? Почему? В каком тексте выше лексическое разнообразие? \n",
    "\n",
    "Таким образом, вам необходимо узнать следующие вещи:\n",
    "\n",
    "- количество предложений\n",
    "- количество токенов\n",
    "- средняя длина предложения (среднее количество слов в предложении)\n",
    "- соотношение \"уникальные токены / все токены\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15e30172-4afa-4bba-8477-afb1599d9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "path = os.path.abspath(os.path.join('..', '..', 'term06', 'BASE', '05-Matplotlib', 'three_comrades.txt'))\n",
    "with open(path, 'r', encoding='ANSI') as file:\n",
    "    raw1 = file.read()\n",
    "with open('three_comrades_deutsch.txt', 'r', encoding='utf8') as file2:\n",
    "    raw2 = file2.read()\n",
    "# sentenizer\n",
    "sent1 = sent_tokenize(raw1)\n",
    "sent2 = sent_tokenize(raw2)\n",
    "# tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+')\n",
    "# len(sentences) \n",
    "len1, len2 = len(sent1), len(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bf6f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#средняя длина предложения\n",
    "def med_len(senten):\n",
    "    tokens = list()\n",
    "    tot_len = 0\n",
    "    for sent in senten:\n",
    "        raw = tokenizer.tokenize(sent)\n",
    "        tokens.extend(raw)\n",
    "        tot_len += len(raw)\n",
    "    med = round(tot_len / len(senten), 2)\n",
    "    return med, tokens\n",
    "\n",
    "\n",
    "def lex_diversity(tokens):\n",
    "    return round(len(set(tokens)) / len(tokens) * 100, 3)\n",
    "\n",
    "\n",
    "med_len1, tokens1 = med_len(sent1)\n",
    "med_len2, tokens2 = med_len(sent2)\n",
    "# len(tokens)\n",
    "tlen1, tlen2 = len(tokens1), len(tokens2)\n",
    "#лексическое разнообразие\n",
    "lex_div_ru = lex_diversity(tokens1)\n",
    "lex_div_de = lex_diversity(tokens2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d81f88d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст на русском. Кол-во предложений:15001.\n",
      "Текст на немецком. Кол-во предложений:9525.\n",
      "\n",
      "Текст на русском. Кол-во слов:114540.\n",
      "Текст на немецком. Кол-во слов:127836.\n",
      "\n",
      "Текст на русском. Средняя длина предложения: 7.64.\n",
      "Текст на русском. Средняя длина предложения:13.42.\n",
      "\n",
      "Текст на русском. Лексическое разнообразие: 19.852.\n",
      "Текст на немецком. Лексическое разнообразие: 10.847\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Текст на русском. Кол-во предложений:{len1}.\\nТекст на немецком. Кол-во предложений:{len2}.\\n')\n",
    "print(f'Текст на русском. Кол-во слов:{tlen1}.\\nТекст на немецком. Кол-во слов:{tlen2}.\\n')\n",
    "print(f'Текст на русском. Средняя длина предложения: {med_len1}.\\nТекст на русском. Средняя длина предложения:{med_len2}.\\n')\n",
    "print(f'Текст на русском. Лексическое разнообразие: {lex_div_ru}.\\nТекст на немецком. Лексическое разнообразие: {lex_div_de}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55495b5e-b16b-4e09-916b-57c8d4c28501",
   "metadata": {},
   "source": [
    "#### Задача 2 (4 балла). \n",
    "\n",
    "Сделайте разборы ваших текстов в формате UD. Посчитайте, какой процент токенов по частям речи имеет совпадающие со словоформой леммы (т.е., в скольких случаях токены с частью речи VERB, например, имели словарную форму: и сам токен, и лемма одинаковые). Что вы можете сказать о выбранных вами языках на основании этих данных? Ожидаются две таблички с процентами совпадающих по лемме и токену слов для каждой части речи. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d931898c-dd07-44ad-aa43-199dfd74ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "doc_ru = nlp(raw1)\n",
    "for token in doc_ru:\n",
    "    t\n",
    "# лемма - словоформа - часть речи\n",
    "# сравнить - словарь (ключ - часть речи)\n",
    "# пандас\n",
    "#\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb9a33-9622-4199-9aad-576b9cb719ba",
   "metadata": {},
   "source": [
    "#### Задача 3 (2 балла). \n",
    "\n",
    "Посчитайте медианную длину предложения для ваших текстов (медиана - это если взять все длины всех ваших предложений, упорядочить их от маленького к большому и выбрать то число, которое оказалось посередине, а если чисел четное количество, то взять среднее арифметическое двух чисел посередине. Например, если у вас пять предложений длинами 1, 2, 6, 7, 8, то медиана - 6, а если шесть предложений длинами 1, 1, 7, 9, 10, 11, то медиана - (7 + 9) / 2 = 8). Возьмите любые два предложения (на разных языках) и постройте для них деревья зависимостей. Изучите связи зависимостей (deprel) и вершины: согласны ли вы с разбором?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93167c-a0b1-4491-a6ad-5f567a2973eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b2561-9f2f-4bf8-89dd-3ec539f3455b",
   "metadata": {},
   "source": [
    "#### Задача 4 (3 балла). \n",
    "\n",
    "Посчитайте частотные списки токенов для каждой категории связей зависимостей (т.е., нужно выделить все токены в тексте, которые получали, например, ярлык amod, и посчитать их частоты). Выведите по первые три самых частотных токена для каждой категории (punct можно не выводить). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce753981-ffa2-4065-914c-e30bb9905f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
